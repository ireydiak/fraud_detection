{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from collections import defaultdict\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Chargement des données\n",
    "Les données sont partagées parmi plusieurs fichiers distincts. L'énoncé nous indique qu'on doit se servir uniquement des fichiers `orders_distance_stores_softmax` et `order_products__prior_specials`, mais il peut être pertinent pour l'analyse d'avoir accès à des informations de base sur les produits tels que le nom et le département de ceux-ci. Nous chargeons donc dans un premier temps tous les fichiers et les concaténons dans un tableau unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Les produits, départements et allées (\"aisles\")\n",
    "product_df = pd.read_csv(\"./data/products.csv\")\n",
    "aisles_df = pd.read_csv(\"./data/aisles.csv\")\n",
    "deparments_df = pd.read_csv(\"./data/departments.csv\")\n",
    "# Fusion des trois tableaux sur les clés primaires\n",
    "products_df = product_df.merge(deparments_df, on=\"department_id\", how=\"left\")\n",
    "products_df = products_df.merge(aisles_df, on=\"aisle_id\", how=\"left\")\n",
    "\n",
    "# Informations sur les commandes (\"orders\")\n",
    "dist_store_df = pd.read_csv(\"./data/orders_distance_stores_softmax.csv\")\n",
    "dist_store_df = dist_store_df.drop(\"Unnamed: 0\", axis=1)\n",
    "prod_prior_df = pd.read_csv(\"./data/order_products__prior_specials.csv\")\n",
    "prod_prior_df = prod_prior_df.drop(\"Unnamed: 0\", axis=1)\n",
    "orders_df = prod_prior_df.merge(dist_store_df, on=\"order_id\")\n",
    "#orders_df = orders_df.merge(products_df, on=\"product_id\", how=\"left\")\n",
    "\n",
    "# Chargement des modèles Word2Vec\n",
    "store_model = Word2Vec.load(\"./models/store_products.model\")\n",
    "product_model = Word2Vec.load(\"./models/products.model\")\n",
    "user_model = Word2Vec.load(\"./models/user_products.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Preprocessing initial\n",
    "Avant de s'attaquer à une sélection astucieuse des données, on peut déjà évacuer les données invalides si elles existent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique columns: ['eval_set']\n",
      "NaN columns: ['days_since_prior_order']\n"
     ]
    }
   ],
   "source": [
    "# Columns with unique values\n",
    "uniq_cols = list(orders_df.columns[orders_df.nunique() == 1])\n",
    "\n",
    "# Columns with invalid values\n",
    "nan_cols = list(orders_df.columns[orders_df.isna().any()])\n",
    "\n",
    "print(\"Unique columns: {}\".format(uniq_cols))\n",
    "print(\"NaN columns: {}\".format(nan_cols))\n",
    "\n",
    "# NaN values days_since_prior_order come from items bought twice on the same day.\n",
    "# Replace the values with with zeros\n",
    "orders_df[\"days_since_prior_order\"] = orders_df[\"days_since_prior_order\"].fillna(0)\n",
    "# Remove columns with unique values\n",
    "orders_df = orders_df.drop(uniq_cols, axis=1)\n",
    "\n",
    "assert len(list(orders_df.columns[orders_df.nunique() == 1])) == 0, \"There are still columns with unique values\"\n",
    "assert len(list(orders_df.columns[orders_df.isna().any()])) == 0, \"There are still columns with NaN values\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target number of rows: 58615\n",
      "Current number of rows: 1172312\n",
      "Number of rows to remove: 1113697\n"
     ]
    },
    {
     "data": {
      "text/plain": "   # Products  # Departments  # Aisles  # Stores  # Users  # Orders\n0       24860             21       134        10     1374    136026",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th># Products</th>\n      <th># Departments</th>\n      <th># Aisles</th>\n      <th># Stores</th>\n      <th># Users</th>\n      <th># Orders</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>24860</td>\n      <td>21</td>\n      <td>134</td>\n      <td>10</td>\n      <td>1374</td>\n      <td>136026</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_products = orders_df[\"product_id\"].nunique()\n",
    "n_stores = orders_df[\"store_id\"].nunique()\n",
    "n_users = orders_df[\"user_id\"].nunique()\n",
    "n_orders = orders_df[\"order_id\"].nunique()\n",
    "n_departments = products_df[\"department_id\"].nunique()\n",
    "n_aisles = products_df[\"aisle_id\"].nunique()\n",
    "n_target_rows = int(len(orders_df) * 0.05)\n",
    "\n",
    "print(\"Target number of rows: %d\" % n_target_rows)\n",
    "print(\"Current number of rows: %d\" % len(orders_df))\n",
    "print(\"Number of rows to remove: %d\" % (len(orders_df) - n_target_rows))\n",
    "basic_info_df = pd.DataFrame(\n",
    "    [[n_products, n_departments, n_aisles, n_stores, n_users, n_orders]],\n",
    "    columns=[\"# Products\", \"# Departments\", \"# Aisles\", \"# Stores\", \"# Users\", \"# Orders\"]\n",
    ")\n",
    "basic_info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    0.774435\n",
      "0    0.225565\n",
      "Name: reordered, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": "<AxesSubplot:>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Distribution des classes\n",
    "print((orders_df[\"reordered\"].value_counts() / len(orders_df)))\n",
    "(orders_df[\"reordered\"].value_counts() / len(orders_df)).plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On constate un débalancement assez sévère des classes. On compte 907 879 (77.4435%) et 264 433 (22.5565%) commandes avec respectivement \"reordered=True\" et \"reordered=False\". Autrement dit, les commandes avec produits achetés à plusieurs reprises sont 3.43 fois plus nombreux que les produits nouveaux. Notre sous-échantillonage doit donc prendre en compte ce débalancement pour éviter de perdre trop d'instances \"reordered=False\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commençons par supprimer les instances aberrantes, c'est-à-dire:\n",
    "- les clients avec peu de commandes différentes;\n",
    "- les produits achetés par peu de clients différents;\n",
    "- les produits présents dans peu de commandes différentes;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction utilitaire permettant de simuler une requête d'aggrégation SQL du genre\n",
    "# SELECT col, COUNT(DISTINCT agg_col) AS n_agg_col FROM table_name GROUP BY col, agg_col\n",
    "# Les commandes équivalentes avec l'API de Pandas génèrent beaucoup de doublons, ce qui nous force\n",
    "# à implémenter cette solution.  \n",
    "def count_distinct(dataframe, groupby_col, count_col):\n",
    "    d = defaultdict()\n",
    "    for row in dataframe.itertuples():\n",
    "        uid = getattr(row, groupby_col)\n",
    "        d[uid] = set()\n",
    "\n",
    "    for row in dataframe.itertuples():\n",
    "        uid = getattr(row, groupby_col)\n",
    "        count_attr = getattr(row, count_col)\n",
    "        d[uid].add(count_attr)\n",
    "\n",
    "    df_count_col_name = \"%s_count\" % count_col\n",
    "    d = {groupby_col: d.keys(), df_count_col_name: [len(v) for v in d.values()]}\n",
    "    return pd.DataFrame.from_dict(d).sort_values(by=df_count_col_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "      user_id  order_id_count\n0       54901              99\n920     71049              99\n919    159610              99\n918    190212              99\n917     19612              99\n...       ...             ...\n454    150481              99\n453    170217              99\n452     31606              99\n460     96192              99\n1373    85247              99\n\n[1374 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>order_id_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>54901</td>\n      <td>99</td>\n    </tr>\n    <tr>\n      <th>920</th>\n      <td>71049</td>\n      <td>99</td>\n    </tr>\n    <tr>\n      <th>919</th>\n      <td>159610</td>\n      <td>99</td>\n    </tr>\n    <tr>\n      <th>918</th>\n      <td>190212</td>\n      <td>99</td>\n    </tr>\n    <tr>\n      <th>917</th>\n      <td>19612</td>\n      <td>99</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>454</th>\n      <td>150481</td>\n      <td>99</td>\n    </tr>\n    <tr>\n      <th>453</th>\n      <td>170217</td>\n      <td>99</td>\n    </tr>\n    <tr>\n      <th>452</th>\n      <td>31606</td>\n      <td>99</td>\n    </tr>\n    <tr>\n      <th>460</th>\n      <td>96192</td>\n      <td>99</td>\n    </tr>\n    <tr>\n      <th>1373</th>\n      <td>85247</td>\n      <td>99</td>\n    </tr>\n  </tbody>\n</table>\n<p>1374 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_n_order_df = count_distinct(orders_df, \"user_id\", \"order_id\")\n",
    "users_n_order_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "       product_id  order_id_count\n24859       17633               1\n21220       36348               1\n21221       19574               1\n21223       29096               1\n21224       33586               1\n...           ...             ...\n123         21903            9594\n126         47209           10654\n27          21137           13163\n62          24852           16466\n37          13176           17652\n\n[24860 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>product_id</th>\n      <th>order_id_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>24859</th>\n      <td>17633</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>21220</th>\n      <td>36348</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>21221</th>\n      <td>19574</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>21223</th>\n      <td>29096</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>21224</th>\n      <td>33586</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>123</th>\n      <td>21903</td>\n      <td>9594</td>\n    </tr>\n    <tr>\n      <th>126</th>\n      <td>47209</td>\n      <td>10654</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>21137</td>\n      <td>13163</td>\n    </tr>\n    <tr>\n      <th>62</th>\n      <td>24852</td>\n      <td>16466</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>13176</td>\n      <td>17652</td>\n    </tr>\n  </tbody>\n</table>\n<p>24860 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products_n_order_df = count_distinct(orders_df, \"product_id\", \"order_id\")\n",
    "products_n_order_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le premier tableau nous informe que les données sont organisées de sorte que chaque utilisateur a exactement 99 commandes. Par conséquent, aucun consommateur ne doit être retiré. Cependant, plusieurs produits sont présents dans une unique commande. Ceux-ci peuvent potentiellement être considérés comme du bruit par nos algorithme de classification et doivent par conséquent être retirés. En ce sens, nous considérons seulement les produits se trouvant dans les 3e et 4e quartiles. Ainsi, on conserve les produits achetés à plusieurs reprises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of products: 24860\n",
      "Number of products to remove: 11983\n",
      "Number of remaining products: 12877\n",
      "Number of rows to remove: 26086\n",
      "Number of remaining rows: 1146226\n",
      "Number of products: 24860\n",
      "Number products to remove: 2740\n",
      "Number of remaining products: 22120\n",
      "Number of rows to remove: 37587\n",
      "Number of remaining rows: 1108639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\verdi\\AppData\\Local\\Temp\\ipykernel_3168\\2057720334.py:30: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  orders_df_prime = orders_df_prime.drop(orders_df_prime[mask].index)\n"
     ]
    }
   ],
   "source": [
    "order_id_cnt_thres = products_n_order_df[\"order_id_count\"].quantile(0.5)\n",
    "n_products = products_n_order_df[\"product_id\"].nunique()\n",
    "mask = products_n_order_df[\"order_id_count\"] < order_id_cnt_thres\n",
    "to_remove = products_n_order_df[mask][\"product_id\"].unique()\n",
    "\n",
    "print(\"Number of products: %d\" % n_products)\n",
    "print(\"Number of products to remove: %d\" % len(to_remove))\n",
    "print(\"Number of remaining products: %d\" % (n_products - len(to_remove)))\n",
    "\n",
    "n_rows_before = len(orders_df)\n",
    "mask = orders_df[\"product_id\"].isin(to_remove)\n",
    "orders_df_prime = orders_df.drop(orders_df[mask].index)\n",
    "n_rows_after = len(orders_df_prime)\n",
    "\n",
    "products_n_users_df = count_distinct(orders_df_prime, \"product_id\", \"user_id\")\n",
    "\n",
    "print(\"Number of rows to remove: %d\" % mask.sum())\n",
    "print(\"Number of remaining rows: %d\" % n_rows_after)\n",
    "\n",
    "user_id_cnt_thres = products_n_users_df[\"user_id_count\"].quantile(0.25)\n",
    "mask = products_n_users_df[\"user_id_count\"] < user_id_cnt_thres\n",
    "to_remove = products_n_users_df[mask][\"product_id\"].unique()\n",
    "\n",
    "print(\"Number of products: %d\" % n_products)\n",
    "print(\"Number products to remove: %d\" % len(to_remove))\n",
    "print(\"Number of remaining products: %d\" % (n_products - len(to_remove)))\n",
    "\n",
    "n_rows_before = len(orders_df_prime)\n",
    "mask = orders_df[\"product_id\"].isin(to_remove)\n",
    "orders_df_prime = orders_df_prime.drop(orders_df_prime[mask].index)\n",
    "n_rows_after = len(orders_df_prime)\n",
    "print(\"Number of rows to remove: %d\" % mask.sum())\n",
    "print(\"Number of remaining rows: %d\" % n_rows_after)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On s'assure que les modifications respectent les conditions, c'est-à-dire\n",
    "- les consommateurs ont plusieurs commandes;\n",
    "- les produits sont présents dans plusieurs commandes;\n",
    "- les produits sont achetés par plusieurs consommateurs;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation\n",
    "users_n_order_df = count_distinct(orders_df_prime, \"user_id\", \"order_id\")\n",
    "assert users_n_order_df[\"order_id_count\"].min() >= order_id_cnt_thres, \"Il reste des utilisateurs avec moins de %d commandes différentes.\" % order_id_cnt_thres\n",
    "\n",
    "products_n_order_df = count_distinct(orders_df_prime, \"product_id\", \"order_id\")\n",
    "assert products_n_order_df[\"order_id_count\"].min() > user_id_cnt_thres, \"Il reste des produits présents dans moins de %d commandes différentes.\" % user_id_cnt_thres\n",
    "\n",
    "products_n_order_df = count_distinct(orders_df_prime, \"product_id\", \"user_id\")\n",
    "assert products_n_order_df[\"user_id_count\"].min() > 1, \"Il reste des produits achetés seulement par un consommateur\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Après toutes ces opérations, les données sont réduites à 1 108 639 observations, ce qui représente environ 5% des données originales. Il reste donc 1 050 023 entrées à supprimer. Pour ce faire, nous proposons la stratégie suivante. On conserve d'abord toutes les instances de la classe minoritaire. On les ordonne ensuite en fonction du nombre de commandes par utilisateur. Finalement, on ajoute une instance originale par utilisateur de la liste obtenue de manière itérative jusqu'à ce que le nombre de lignes souhaitées (5% des données originales) soit obtenu.\n",
    "\n",
    "Stratégie alternative: puisque nous avons un nombre de commandes élevées pour chacun des utilisateurs, on peut simplement conserver au plus $\\tau = n\\_lignes\\_disponibles / n\\_clients$  commandes différentes pour chacun des utilisateurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum number of orders per user: 43\n"
     ]
    }
   ],
   "source": [
    "tau = int(np.ceil(n_target_rows / users_n_order_df[\"user_id\"].nunique()))\n",
    "assert users_n_order_df[\"order_id_count\"].min() > tau, \"Il existe un ou plusieurs clients avec un nombre de commandes inférieur à %d.\" % tau\n",
    "print(\"Maximum number of orders per user: %d\" % tau)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme tous les clients possèdent au minimum 42 commandes, cette stratégie devrait fonctionner. On doit être prudent et conserver un ratio des classes similaire dans notre procédure. Avec la loi forte des grands nombres, on sait qu'un sous-échantillonage aléatoire suffisamment grand devrait conserver le ratio original. Toutefois, pour nous en assurer, nous introduisons une procédure de type Las Vegas qui répète le sous-échantillonage jusqu'à ce le ratio soit respecté à un epsilon près. Une procédure aléatoire semble inappropriée car on cherche des paniers d'achat variés. Toutefois, considérant le faible nombre de paniers par consommateurs (99) et le nombre de paniers conservés (42), la probabilité de ne pas piger des produits variés est négligeable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    0.7756\n",
      "0    0.2244\n",
      "Name: reordered, dtype: float64\n",
      "Processus complété avec 1 itération(s)\n"
     ]
    }
   ],
   "source": [
    "# On s'assure que le balancement des classes demeure le même\n",
    "class_ratio_df = orders_df_prime[\"reordered\"].value_counts() / len(orders_df_prime)\n",
    "maj_cls_ratio = class_ratio_df[1]\n",
    "min_cls_ratio = class_ratio_df[0]\n",
    "\n",
    "ratio_is_respected, n_iter = False, 0\n",
    "final_df = pd.DataFrame(columns=orders_df_prime.columns)\n",
    "class_ratio_final_df = None\n",
    "\n",
    "while not ratio_is_respected:\n",
    "    final_df = pd.DataFrame(columns=orders_df_prime.columns)\n",
    "    for tup in users_n_order_df.itertuples():\n",
    "        # Masque pour filtrer l'utilisateur\n",
    "        mask = orders_df_prime[\"user_id\"] == tup.user_id\n",
    "        # Échantillonnage aléatoire des commandes de l'utilisateur\n",
    "        random_orders = orders_df_prime[mask].sample(tau)\n",
    "        # Ajout des échantillons au tableau final\n",
    "        final_df = pd.concat((final_df, random_orders))\n",
    "    # Calcul du ratio des classes\n",
    "    class_ratio_final_df = final_df[\"reordered\"].value_counts() / len(final_df)\n",
    "    ratio_is_respected = np.isclose(class_ratio_final_df[1], maj_cls_ratio, atol=1e-2) and np.isclose(class_ratio_final_df[0], min_cls_ratio, atol=1e-2)\n",
    "    n_iter += 1\n",
    "\n",
    "# On réordonne l'ordre des colonnes pour avoir les identifiants uniques au début et l'étiquette recherchée à la fin\n",
    "first_cols = [\"order_id\", \"product_id\", \"user_id\", \"store_id\"]\n",
    "last_col = [\"reordered\"]\n",
    "other_cols = list(set(final_df.columns) - set(last_col) - set(first_cols))\n",
    "final_df = final_df.loc[:, first_cols + other_cols + last_col]\n",
    "final_df\n",
    "# Sauvegarde des données finales\n",
    "final_df.to_csv(\"./data/train_orders.csv\", index=False)\n",
    "\n",
    "print(class_ratio_final_df)\n",
    "print(\"Processus complété avec %d itération(s)\" % n_iter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3075153381.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;36m  File \u001B[1;32m\"C:\\Users\\verdi\\AppData\\Local\\Temp\\ipykernel_3168\\3075153381.py\"\u001B[1;36m, line \u001B[1;32m2\u001B[0m\n\u001B[1;33m    Les attributs \"user_id\", \"store_id\", \"product_id\" sont chargés et interprétés comme des données numériques par nos librairies, mais ils correspondent en fait en des attributs catégoriques. En effet, ceux servent à identifier les observations et non pas pour effectuer des opérations arithmétiques. Par exemple, la différence entre \"user_id=1\" et \"user_id=10000\" peut être numériquement grande ($10000 - 1 = 9999$ dans ce cas), mais ne traduit pas la différence réelle entre ces deux utilisateurs. Peut-être que ceux-ci achètent en fait les mêmes produits et sont similaires en termes de produits achetés. Par conséquent, nous devons convertir les attributs \"user_id\", \"store_id\" et \"product_id\" en valeurs numériques afin que ceux-ci soient compatibles avec nos modèles.\u001B[0m\n\u001B[1;37m        ^\u001B[0m\n\u001B[1;31mSyntaxError\u001B[0m\u001B[1;31m:\u001B[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "def w2v_emb(df: pd.DataFrame, groupby_col: str, aggregate_col: str, n_epochs=5, embedding_size=10, num_negative_samples=7, min_count=1):\n",
    "    df = df.groupby(groupby_col).apply(lambda x: x[aggregate_col].tolist())\n",
    "    window_size = np.max(df.apply(len))\n",
    "\n",
    "    # Entraînement du modèle\n",
    "    return gensim.models.Word2Vec(\n",
    "        sentences=df,\n",
    "        sg=1, # 1 for skip-gram\n",
    "        vector_size=embedding_size,\n",
    "        window=window_size,\n",
    "        min_count=min_count,\n",
    "        workers=4,\n",
    "        hs=0, # negative sampling will be used.\n",
    "        negative=num_negative_samples,\n",
    "        ns_exponent=0, # The exponent used to shape the negative sampling distribution.\n",
    "        epochs=n_epochs\n",
    "    )\n",
    "\n",
    "product_model = w2v_emb(final_df, \"order_id\", \"product_id\")\n",
    "user_model = w2v_emb(final_df, \"product_id\", \"user_id\")\n",
    "store_model = w2v_emb(final_df, \"product_id\", \"store_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def add_embeddings(df: pd.DataFrame, w2v_model: Word2Vec, key: str):\n",
    "    embeddings = []\n",
    "    for row in df.itertuples():\n",
    "        val = getattr(row, key)\n",
    "        embed = w2v_model.wv.get_vector(val, norm=True)\n",
    "        embeddings.append(embed)\n",
    "    return np.array(embeddings)\n",
    "\n",
    "def concat_df_cols(original_df: pd.DataFrame, to_add: np.ndarray, suffix: str):\n",
    "    assert to_add.ndim == 2, \"to_add must have exactly 2 dimensions\" \n",
    "    to_add_df = pd.DataFrame(to_add, columns=[\"%s%d\" % (suffix, i) for i in range(to_add.shape[1])])\n",
    "    merged_df = pd.concat(\n",
    "        (original_df.reset_index(), to_add_df),\n",
    "        axis=1\n",
    "    )\n",
    "    return merged_df.drop(\"index\", axis=1, errors=\"ignore\")\n",
    "\n",
    "final_df = concat_df_cols(final_df, add_embeddings(final_df, product_model, \"product_id\"), \"P\")\n",
    "final_df = concat_df_cols(final_df, add_embeddings(final_df, user_model, \"user_id\"), \"U\")\n",
    "final_df = concat_df_cols(final_df, add_embeddings(final_df, store_model, \"store_id\"), \"S\")\n",
    "to_drop = [\"order_id\", \"product_id\", \"user_id\", \"store_id\"]\n",
    "final_df = final_df.drop(to_drop, axis=1)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "to_scale = ['special', 'order_dow', 'order_number', 'days_since_prior_order', 'order_hour_of_day', 'distance', 'add_to_cart_order', 'reordered']\n",
    "final_df[to_scale] = scaler.fit_transform(final_df[to_scale])\n",
    "final_df = final_df[final_df.drop(\"reordered\", axis=1).columns.tolist() + [\"reordered\"]]\n",
    "final_df.to_csv(\"./train_orders.csv\", index=False)\n",
    "data = final_df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, Subset, DataLoader\n",
    "from torch.utils.data.dataset import T_co\n",
    "from typing import Tuple\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class InstaCartDataset(Dataset):\n",
    "    def __init__(self, data: np.array, val_ratio: float=0.1, test_ratio: float=0.33, batch_size: int = 32):\n",
    "        self.name = self.__class__.__name__\n",
    "        self.X = data[:, :-1]\n",
    "        self.y = data[:, -1]\n",
    "        self.val_ratio = val_ratio\n",
    "        self.test_ratio = test_ratio\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.n_instances = self.X.shape[0]\n",
    "        self.in_features = self.X.shape[1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, index) -> T_co:\n",
    "        return self.X[index], self.y[index]\n",
    "\n",
    "    def loaders(self, num_workers: int = 4, seed: int = None) -> Tuple[DataLoader, DataLoader, DataLoader]:\n",
    "        X_train, X_val, y_train, y_val = train_test_split(self.X, self.y, test_size=self.val_ratio)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=self.test_ratio)\n",
    "        train_ldr = DataLoader(dataset=X_train, batch_size=self.batch_size, num_workers=num_workers)\n",
    "        val_ldr = DataLoader(dataset=X_val, batch_size=self.batch_size, num_workers=num_workers)\n",
    "        test_ldr = DataLoader(dataset=X_test, batch_size=self.batch_size, num_workers=num_workers)\n",
    "        return train_ldr, val_ldr, test_ldr\n",
    "\n",
    "    def split_train_test(self, test_pct: float = .5, label: int = 0, seed=None) -> Tuple[Subset, Subset]:\n",
    "        assert (label == 0 or label == 1)\n",
    "\n",
    "        if seed:\n",
    "            torch.manual_seed(seed)\n",
    "\n",
    "        # Fetch and shuffle indices of a single class\n",
    "        label_data_idx = np.where(self.y == label)[0]\n",
    "        shuffled_idx = torch.randperm(len(label_data_idx)).long()\n",
    "\n",
    "        # Generate training set\n",
    "        num_test_sample = int(len(label_data_idx) * test_pct)\n",
    "        num_train_sample = int(len(label_data_idx) * (1. - test_pct))\n",
    "        train_set = Subset(self, label_data_idx[shuffled_idx[num_train_sample:]])\n",
    "\n",
    "        # Generate test set based on the remaining data and the previously filtered out labels\n",
    "        remaining_idx = np.concatenate([\n",
    "            label_data_idx[shuffled_idx[:num_test_sample]],\n",
    "            np.where(self.y == int(not label))[0]\n",
    "        ])\n",
    "        test_set = Subset(self, remaining_idx)\n",
    "\n",
    "        return train_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "from tqdm import tqdm \n",
    "\n",
    "\n",
    "class MLP5(nn.Module):\n",
    "    name = \"MLP5\"\n",
    "\n",
    "    def __init__(self, in_features: int, n_class: int = 2, p_dropout=0.1):\n",
    "        super(MLP5, self).__init__()\n",
    "\n",
    "        self.in_layer = nn.Linear(in_features, 1024)\n",
    "        self.h1 = nn.Linear(1024, 1024)\n",
    "        self.h2 = nn.Linear(1024, 512)\n",
    "        self.h3 = nn.Linear(512, 256)\n",
    "        self.h4 = nn.Linear(256, 128)\n",
    "        self.h5 = nn.Linear(128, 64)\n",
    "        self.out_layer = nn.Linear(64, n_class - 1)\n",
    "        self.dropout = nn.Dropout(p_dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, X: torch.Tensor):\n",
    "        out = self.relu(self.in_layer(X))\n",
    "        out = self.relu(self.h1(out))\n",
    "        out = self.relu(self.h2(out))\n",
    "        out = self.relu(self.h3(out))\n",
    "        out = self.relu(self.h4(out))\n",
    "        out = self.relu(self.h5(out))\n",
    "        out = self.dropout(out)\n",
    "        out = self.out_layer(out)\n",
    "        return out\n",
    "\n",
    "class MLP3(nn.Module):\n",
    "    name = \"MLP3\"\n",
    "\n",
    "    def __init__(self, in_features: int, n_class: int = 2, p_dropout=0.1):\n",
    "        super(MLP3, self).__init__()\n",
    "\n",
    "        self.in_layer = nn.Linear(in_features, 256)\n",
    "        self.h2 = nn.Linear(256, 256)\n",
    "        self.h3 = nn.Linear(256, 128)\n",
    "        self.h4 = nn.Linear(128, 64)\n",
    "        self.out_layer = nn.Linear(64, n_class - 1)\n",
    "        self.dropout = nn.Dropout(p_dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, X: torch.Tensor):\n",
    "        out = self.relu(self.in_layer(X))\n",
    "        out = self.relu(self.h2(out))\n",
    "        out = self.relu(self.h3(out))\n",
    "        out = self.relu(self.h4(out))\n",
    "        out = self.dropout(out)\n",
    "        out = self.out_layer(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class MLP2(nn.Module):\n",
    "    name = \"MLP2\"\n",
    "\n",
    "    def __init__(self, in_features: int, n_class: int = 2, p_dropout=0.1):\n",
    "        super(MLP2, self).__init__()\n",
    "\n",
    "        self.h1 = nn.Linear(in_features, in_features // 2)\n",
    "        self.h2 = nn.Linear(in_features // 2, in_features // 2)\n",
    "        self.h3 = nn.Linear(in_features // 2, in_features // 4)\n",
    "        self.h4 = nn.Linear(in_features // 4, n_class - 1)\n",
    "        self.dropout = nn.Dropout(p_dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, X: torch.Tensor):\n",
    "        out = self.relu(self.h1(X))\n",
    "        out = self.relu(self.h2(out))\n",
    "        out = self.relu(self.h3(out))\n",
    "        out = self.relu(self.h4(out))\n",
    "        out = self.relu(self.h5(out))\n",
    "        out = self.dropout(out)\n",
    "        out = self.h6(out)\n",
    "        return out\n",
    "\n",
    "class MLPTrainer:\n",
    "    def __init__(self, model, optimizer, device: str = \"cuda\"):\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    def train(self, dataset_ldr, num_epochs):\n",
    "        \"\"\"\n",
    "        Train the model for num_epochs times\n",
    "        Args:\n",
    "            num_epochs: number times to train the model\n",
    "        \"\"\"\n",
    "\n",
    "        # Create pytorch's train data_loader\n",
    "        train_loader = dataset_ldr\n",
    "\n",
    "        # train num_epochs times\n",
    "        for epoch in range(num_epochs):\n",
    "            print(\"Epoch: {} of {}\".format(epoch + 1, num_epochs))\n",
    "            train_loss = 0.0\n",
    "            self.cur_epoch = epoch\n",
    "\n",
    "            with tqdm(range(len(train_loader))) as t:\n",
    "                train_losses = []\n",
    "                for i, data in enumerate(train_loader, 0):\n",
    "                    # transfer tensors to selected device\n",
    "                    train_inputs = data[0].to(self.device, dtype=torch.float)\n",
    "                    train_labels = data[1].to(self.device, dtype=torch.long)\n",
    "                    # zero the parameter gradients\n",
    "                    self.optimizer.zero_grad()\n",
    "\n",
    "                    # forward pass\n",
    "                    train_outputs = self.model(train_inputs)\n",
    "                    # compute loss\n",
    "                    loss = self.loss_fn(train_outputs, train_labels)\n",
    "\n",
    "                    # Use autograd to compute the backward pass.\n",
    "                    loss.backward()\n",
    "                    self.cur_loss = loss.item()\n",
    "                    self.optimizer.step()\n",
    "\n",
    "                    # Save losses for plotting purposes\n",
    "                    train_losses.append(loss.item())\n",
    "\n",
    "                    # print metrics along progress bar\n",
    "                    train_loss += loss.item()\n",
    "                    t.set_postfix(loss='{:05.3f}'.format(train_loss / (i + 1)))\n",
    "                    t.update()\n",
    "    \n",
    "            # evaluate the model on validation data after each epoch\n",
    "            self.metric_values['train_loss'].append(np.mean(train_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = InstaCartDataset(data)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "n_epochs = 200\n",
    "train_ldr, val_ldr, test_ldr = dataset.loaders()\n",
    " \n",
    "for model_cls in [MLP5, MLP3, MLP2]:\n",
    "    model = model_cls(in_features=dataset.in_features)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    trainer = MLPTrainer(model=model, optimizer=optimizer, device=device)\n",
    "    trainer.train(train_ldr, n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Le modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L'entraînement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L'évaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/18138693/replicating-group-concat-for-pandas-dataframe\n",
    "P = np.zeros((n_products, n_products))\n",
    "order_prod_grouped_df = orders_df.groupby(\"order_id\").apply(lambda x: list(x[\"product_id\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "order_prod_grouped_df"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "98721178b2fa100978e69bf0600317bd2bb664c245b34d5c4cc5c099a20743a9"
  },
  "kernelspec": {
   "display_name": "Python 3.6.15 ('ift780')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}