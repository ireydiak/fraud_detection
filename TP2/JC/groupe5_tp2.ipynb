{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Chargement des données\n",
    "Les données sont partagées parmi plusieurs fichiers distincts. L'énoncé nous indique qu'on doit se servir uniquement des fichiers `orders_distance_stores_softmax` et `order_products__prior_specials`, mais il peut être pertinent pour l'analyse d'avoir accès à des informations de base sur les produits tels que le nom et le département de ceux-ci. Nous chargeons donc dans un premier temps tous les fichiers et les concaténons dans un tableau unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Les produits, départements et allées (\"aisles\")\n",
    "product_df = pd.read_csv(\"./data/products.csv\")\n",
    "aisles_df = pd.read_csv(\"./data/aisles.csv\")\n",
    "deparments_df = pd.read_csv(\"./data/departments.csv\")\n",
    "# Fusion des trois tableaux sur les clés primaires\n",
    "products_df = product_df.merge(deparments_df, on=\"department_id\", how=\"left\")\n",
    "products_df = products_df.merge(aisles_df, on=\"aisle_id\", how=\"left\")\n",
    "\n",
    "# Informations sur les commandes (\"orders\")\n",
    "dist_store_df = pd.read_csv(\"./data/orders_distance_stores_softmax.csv\")\n",
    "dist_store_df = dist_store_df.drop(\"Unnamed: 0\", axis=1)\n",
    "prod_prior_df = pd.read_csv(\"./data/order_products__prior_specials.csv\")\n",
    "prod_prior_df = prod_prior_df.drop(\"Unnamed: 0\", axis=1)\n",
    "orders_df = prod_prior_df.merge(dist_store_df, on=\"order_id\")\n",
    "#orders_df = orders_df.merge(products_df, on=\"product_id\", how=\"left\")\n",
    "\n",
    "# Chargement des modèles Word2Vec\n",
    "store_model = Word2Vec.load(\"./models/store_products.model\")\n",
    "product_model = Word2Vec.load(\"./models/products.model\")\n",
    "user_model = Word2Vec.load(\"./models/user_products.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Preprocessing\n",
    "## Données invalides\n",
    "Avant de s'attaquer à une sélection astucieuse des données, on peut déjà évacuer les données invalides si elles existent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique columns: ['eval_set']\n",
      "NaN columns: ['days_since_prior_order']\n"
     ]
    }
   ],
   "source": [
    "# Columns with unique values\n",
    "uniq_cols = list(orders_df.columns[orders_df.nunique() == 1])\n",
    "\n",
    "# Columns with invalid values\n",
    "nan_cols = list(orders_df.columns[orders_df.isna().any()])\n",
    "\n",
    "print(\"Unique columns: {}\".format(uniq_cols))\n",
    "print(\"NaN columns: {}\".format(nan_cols))\n",
    "\n",
    "# NaN values days_since_prior_order come from items bought twice on the same day.\n",
    "# Replace the values with with zeros\n",
    "orders_df[\"days_since_prior_order\"] = orders_df[\"days_since_prior_order\"].fillna(0)\n",
    "# Remove columns with unique values\n",
    "orders_df = orders_df.drop(uniq_cols, axis=1)\n",
    "\n",
    "assert len(list(orders_df.columns[orders_df.nunique() == 1])) == 0, \"There are still columns with unique values\"\n",
    "assert len(list(orders_df.columns[orders_df.isna().any()])) == 0, \"There are still columns with NaN values\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target number of rows: 58615\n",
      "Current number of rows: 1172312\n",
      "Number of rows to remove: 1113697\n"
     ]
    },
    {
     "data": {
      "text/plain": "   # Products  # Departments  # Aisles  # Stores  # Users  # Orders\n0       24860             21       134        10     1374    136026",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th># Products</th>\n      <th># Departments</th>\n      <th># Aisles</th>\n      <th># Stores</th>\n      <th># Users</th>\n      <th># Orders</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>24860</td>\n      <td>21</td>\n      <td>134</td>\n      <td>10</td>\n      <td>1374</td>\n      <td>136026</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_products = orders_df[\"product_id\"].nunique()\n",
    "n_stores = orders_df[\"store_id\"].nunique()\n",
    "n_users = orders_df[\"user_id\"].nunique()\n",
    "n_orders = orders_df[\"order_id\"].nunique()\n",
    "n_departments = products_df[\"department_id\"].nunique()\n",
    "n_aisles = products_df[\"aisle_id\"].nunique()\n",
    "n_target_rows = int(len(orders_df) * 0.05)\n",
    "\n",
    "print(\"Target number of rows: %d\" % n_target_rows)\n",
    "print(\"Current number of rows: %d\" % len(orders_df))\n",
    "print(\"Number of rows to remove: %d\" % (len(orders_df) - n_target_rows))\n",
    "basic_info_df = pd.DataFrame(\n",
    "    [[n_products, n_departments, n_aisles, n_stores, n_users, n_orders]],\n",
    "    columns=[\"# Products\", \"# Departments\", \"# Aisles\", \"# Stores\", \"# Users\", \"# Orders\"]\n",
    ")\n",
    "basic_info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    0.774435\n",
      "0    0.225565\n",
      "Name: reordered, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": "<AxesSubplot:>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Distribution des classes\n",
    "print((orders_df[\"reordered\"].value_counts() / len(orders_df)))\n",
    "(orders_df[\"reordered\"].value_counts() / len(orders_df)).plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On constate un débalancement assez sévère des classes. On compte 907 879 (77.4435%) et 264 433 (22.5565%) commandes avec respectivement \"reordered=True\" et \"reordered=False\". Autrement dit, les commandes avec produits achetés à plusieurs reprises sont 3.43 fois plus nombreuses que les commandes avec des produits nouveaux. Notre sous-échantillonnage doit prendre en compte ce débalancement pour éviter de perdre trop d'instances \"reordered=False\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commençons par supprimer les instances aberrantes, c'est-à-dire:\n",
    "- les clients avec peu de commandes différentes;\n",
    "- les produits achetés par peu de clients différents;\n",
    "- les produits présents dans peu de commandes différentes;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction utilitaire permettant de simuler une requête d'aggrégation SQL du genre\n",
    "# SELECT col, COUNT(DISTINCT agg_col) AS n_agg_col FROM table_name GROUP BY col, agg_col\n",
    "# Les commandes équivalentes avec l'API de Pandas génèrent beaucoup de doublons, ce qui nous force\n",
    "# à implémenter cette solution.  \n",
    "def count_distinct(dataframe, groupby_col, count_col):\n",
    "    d = defaultdict()\n",
    "    for row in dataframe.itertuples():\n",
    "        uid = getattr(row, groupby_col)\n",
    "        d[uid] = set()\n",
    "\n",
    "    for row in dataframe.itertuples():\n",
    "        uid = getattr(row, groupby_col)\n",
    "        count_attr = getattr(row, count_col)\n",
    "        d[uid].add(count_attr)\n",
    "\n",
    "    df_count_col_name = \"%s_count\" % count_col\n",
    "    d = {groupby_col: d.keys(), df_count_col_name: [len(v) for v in d.values()]}\n",
    "    return pd.DataFrame.from_dict(d).sort_values(by=df_count_col_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "      user_id  order_id_count\n0       54901              99\n920     71049              99\n919    159610              99\n918    190212              99\n917     19612              99\n...       ...             ...\n454    150481              99\n453    170217              99\n452     31606              99\n460     96192              99\n1373    85247              99\n\n[1374 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>order_id_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>54901</td>\n      <td>99</td>\n    </tr>\n    <tr>\n      <th>920</th>\n      <td>71049</td>\n      <td>99</td>\n    </tr>\n    <tr>\n      <th>919</th>\n      <td>159610</td>\n      <td>99</td>\n    </tr>\n    <tr>\n      <th>918</th>\n      <td>190212</td>\n      <td>99</td>\n    </tr>\n    <tr>\n      <th>917</th>\n      <td>19612</td>\n      <td>99</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>454</th>\n      <td>150481</td>\n      <td>99</td>\n    </tr>\n    <tr>\n      <th>453</th>\n      <td>170217</td>\n      <td>99</td>\n    </tr>\n    <tr>\n      <th>452</th>\n      <td>31606</td>\n      <td>99</td>\n    </tr>\n    <tr>\n      <th>460</th>\n      <td>96192</td>\n      <td>99</td>\n    </tr>\n    <tr>\n      <th>1373</th>\n      <td>85247</td>\n      <td>99</td>\n    </tr>\n  </tbody>\n</table>\n<p>1374 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_n_order_df = count_distinct(orders_df, \"user_id\", \"order_id\")\n",
    "users_n_order_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "       product_id  order_id_count\n24859       17633               1\n21220       36348               1\n21221       19574               1\n21223       29096               1\n21224       33586               1\n...           ...             ...\n123         21903            9594\n126         47209           10654\n27          21137           13163\n62          24852           16466\n37          13176           17652\n\n[24860 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>product_id</th>\n      <th>order_id_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>24859</th>\n      <td>17633</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>21220</th>\n      <td>36348</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>21221</th>\n      <td>19574</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>21223</th>\n      <td>29096</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>21224</th>\n      <td>33586</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>123</th>\n      <td>21903</td>\n      <td>9594</td>\n    </tr>\n    <tr>\n      <th>126</th>\n      <td>47209</td>\n      <td>10654</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>21137</td>\n      <td>13163</td>\n    </tr>\n    <tr>\n      <th>62</th>\n      <td>24852</td>\n      <td>16466</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>13176</td>\n      <td>17652</td>\n    </tr>\n  </tbody>\n</table>\n<p>24860 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products_n_order_df = count_distinct(orders_df, \"product_id\", \"order_id\")\n",
    "products_n_order_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le premier tableau nous informe que les données sont organisées de sorte que chaque utilisateur a exactement 99 commandes. Par conséquent, aucun consommateur ne doit être retiré. Cependant, plusieurs produits sont présents dans une unique commande. Ceux-ci peuvent potentiellement être considérés comme du bruit par nos algorithme de classification et doivent par conséquent être retirés. En ce sens, nous considérons seulement les produits se trouvant dans les 3e et 4e quartiles. Ainsi, on conserve les produits achetés à plusieurs reprises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of products: 24860\n",
      "Number of products to remove: 11983\n",
      "Number of remaining products: 12877\n",
      "Number of rows to remove: 26086\n",
      "Number of remaining rows: 1146226\n",
      "Number of products: 24860\n",
      "Number products to remove: 2740\n",
      "Number of remaining products: 22120\n",
      "Number of rows to remove: 37587\n",
      "Number of remaining rows: 1108639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\verdi\\AppData\\Local\\Temp\\ipykernel_14320\\2057720334.py:30: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  orders_df_prime = orders_df_prime.drop(orders_df_prime[mask].index)\n"
     ]
    }
   ],
   "source": [
    "order_id_cnt_thres = products_n_order_df[\"order_id_count\"].quantile(0.5)\n",
    "n_products = products_n_order_df[\"product_id\"].nunique()\n",
    "mask = products_n_order_df[\"order_id_count\"] < order_id_cnt_thres\n",
    "to_remove = products_n_order_df[mask][\"product_id\"].unique()\n",
    "\n",
    "print(\"Number of products: %d\" % n_products)\n",
    "print(\"Number of products to remove: %d\" % len(to_remove))\n",
    "print(\"Number of remaining products: %d\" % (n_products - len(to_remove)))\n",
    "\n",
    "n_rows_before = len(orders_df)\n",
    "mask = orders_df[\"product_id\"].isin(to_remove)\n",
    "orders_df_prime = orders_df.drop(orders_df[mask].index)\n",
    "n_rows_after = len(orders_df_prime)\n",
    "\n",
    "products_n_users_df = count_distinct(orders_df_prime, \"product_id\", \"user_id\")\n",
    "\n",
    "print(\"Number of rows to remove: %d\" % mask.sum())\n",
    "print(\"Number of remaining rows: %d\" % n_rows_after)\n",
    "\n",
    "user_id_cnt_thres = products_n_users_df[\"user_id_count\"].quantile(0.25)\n",
    "mask = products_n_users_df[\"user_id_count\"] < user_id_cnt_thres\n",
    "to_remove = products_n_users_df[mask][\"product_id\"].unique()\n",
    "\n",
    "print(\"Number of products: %d\" % n_products)\n",
    "print(\"Number products to remove: %d\" % len(to_remove))\n",
    "print(\"Number of remaining products: %d\" % (n_products - len(to_remove)))\n",
    "\n",
    "n_rows_before = len(orders_df_prime)\n",
    "mask = orders_df[\"product_id\"].isin(to_remove)\n",
    "orders_df_prime = orders_df_prime.drop(orders_df_prime[mask].index)\n",
    "n_rows_after = len(orders_df_prime)\n",
    "print(\"Number of rows to remove: %d\" % mask.sum())\n",
    "print(\"Number of remaining rows: %d\" % n_rows_after)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On s'assure que les modifications respectent les conditions, c'est-à-dire\n",
    "- les consommateurs ont plusieurs commandes;\n",
    "- les produits sont présents dans plusieurs commandes;\n",
    "- les produits sont achetés par plusieurs consommateurs;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation\n",
    "users_n_order_df = count_distinct(orders_df_prime, \"user_id\", \"order_id\")\n",
    "assert users_n_order_df[\"order_id_count\"].min() >= order_id_cnt_thres, \"Il reste des utilisateurs avec moins de %d commandes différentes.\" % order_id_cnt_thres\n",
    "\n",
    "products_n_order_df = count_distinct(orders_df_prime, \"product_id\", \"order_id\")\n",
    "assert products_n_order_df[\"order_id_count\"].min() > user_id_cnt_thres, \"Il reste des produits présents dans moins de %d commandes différentes.\" % user_id_cnt_thres\n",
    "\n",
    "products_n_order_df = count_distinct(orders_df_prime, \"product_id\", \"user_id\")\n",
    "assert products_n_order_df[\"user_id_count\"].min() > 1, \"Il reste des produits achetés seulement par un consommateur\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Après toutes ces opérations, les données sont réduites à 1 108 639 observations, ce qui représente environ 5% des données originales. Il reste donc 1 050 023 entrées à supprimer. Pour ce faire, nous proposons la stratégie suivante. Puisque nous avons un nombre de commandes élevé pour chacun des utilisateurs, on peut simplement conserver au plus $\\tau = n\\_commandes\\_disponibles / n\\_clients$ commandes différentes pour chacun des utilisateurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum number of orders per user: 43\n"
     ]
    }
   ],
   "source": [
    "tau = int(np.ceil(n_target_rows / users_n_order_df[\"user_id\"].nunique()))\n",
    "assert users_n_order_df[\"order_id_count\"].min() > tau, \"Il existe un ou plusieurs clients avec un nombre de commandes inférieur à %d.\" % tau\n",
    "print(\"Maximum number of orders per user: %d\" % tau)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme tous les clients possèdent au minimum 43 commandes, cette stratégie devrait fonctionner. On doit être prudent et conserver un ratio des classes similaire dans notre procédure. Avec la loi forte des grands nombres, on sait qu'un sous-échantillonage aléatoire suffisamment grand devrait conserver le ratio original. Toutefois, pour nous en assurer, nous introduisons une procédure de type Las Vegas qui répète le sous-échantillonage jusqu'à ce le ratio soit respecté à un epsilon près. Une procédure aléatoire semble inappropriée car on cherche des paniers d'achat variés. Toutefois, considérant le faible nombre de paniers par consommateurs (99) et le nombre de paniers conservés (43), la probabilité de ne pas piger des produits variés est négligeable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    0.775058\n",
      "0    0.224942\n",
      "Name: reordered, dtype: float64\n",
      "Processus complété avec 2 itération(s)\n"
     ]
    }
   ],
   "source": [
    "# On s'assure que le balancement des classes demeure le même\n",
    "class_ratio_df = orders_df_prime[\"reordered\"].value_counts() / len(orders_df_prime)\n",
    "maj_cls_ratio = class_ratio_df[1]\n",
    "min_cls_ratio = class_ratio_df[0]\n",
    "\n",
    "ratio_is_respected, n_iter = False, 0\n",
    "final_df = pd.DataFrame(columns=orders_df_prime.columns)\n",
    "class_ratio_final_df = None\n",
    "\n",
    "while not ratio_is_respected:\n",
    "    final_df = pd.DataFrame(columns=orders_df_prime.columns)\n",
    "    for tup in users_n_order_df.itertuples():\n",
    "        # Masque pour filtrer l'utilisateur\n",
    "        mask = orders_df_prime[\"user_id\"] == tup.user_id\n",
    "        # Échantillonnage aléatoire des commandes de l'utilisateur\n",
    "        random_orders = orders_df_prime[mask].sample(tau)\n",
    "        # Ajout des échantillons au tableau final\n",
    "        final_df = pd.concat((final_df, random_orders))\n",
    "    # Calcul du ratio des classes\n",
    "    class_ratio_final_df = final_df[\"reordered\"].value_counts() / len(final_df)\n",
    "    ratio_is_respected = np.isclose(class_ratio_final_df[1], maj_cls_ratio, atol=1e-2) and np.isclose(class_ratio_final_df[0], min_cls_ratio, atol=1e-2)\n",
    "    n_iter += 1\n",
    "\n",
    "print(class_ratio_final_df)\n",
    "print(\"Processus complété avec %d itération(s)\" % n_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Conversion des variables catégoriques\n",
    "Les attributs `product_id`, `user_id` et `store_id` sont initialement chargés et interprétés comme étant des valeurs numériques par nos librairies. Toutefois, ceux-ci représentent des identifiants uniques qui ne se soumettent pas à des opérations arithmétiques. Par exemple, la distance numérique entre les utilisateurs $1000$ et $1$ ($1000-1=999$) ne fait aucun sens, car on compare leur identifiant. Peut-être achètent-ils les mêmes produits et magasinent au même endroit et sont en fait beaucoup plus similaires. Par conséquent, nous empruntons une stratégie tirée des langages naturels pour convertir des mots en vecteurs permettant de représenter une notion de distances entre ceux-ci. Nous utilisons à cet effet l'algorithme `Word2Vec` offert par la librairie Gensim. Ainsi, les attributs `user_id`, `store_id` et `product_id` sont convertis en vecteurs normés de $10$ éléments chacun. Pour ce faire on génère trois tableaux de contingence: le premier associe les produits à chaque commande, le deuxième chaque utilisateur aux produits et le dernier chaque magasin à chaque produit. Finalement, on normalise les autres attributs et on se débarrasse des colonnes `user_id`, `store_id`, `product_id` et `order_id` puisqu'elles sont désormais désuettes."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def w2v_emb(df: pd.DataFrame, groupby_col: str, aggregate_col: str, n_epochs=5, embedding_size=10, num_negative_samples=7, min_count=1):\n",
    "    df = df.groupby(groupby_col).apply(lambda x: x[aggregate_col].tolist())\n",
    "    window_size = np.max(df.apply(len))\n",
    "\n",
    "    # Entraînement du modèle\n",
    "    return gensim.models.Word2Vec(\n",
    "        sentences=df,\n",
    "        sg=1, # 1 for skip-gram\n",
    "        vector_size=embedding_size,\n",
    "        window=window_size,\n",
    "        min_count=min_count,\n",
    "        workers=4,\n",
    "        hs=0, # negative sampling will be used.\n",
    "        negative=num_negative_samples,\n",
    "        ns_exponent=0, # The exponent used to shape the negative sampling distribution.\n",
    "        epochs=n_epochs\n",
    "    )\n",
    "\n",
    "def add_embeddings(df: pd.DataFrame, w2v_model: Word2Vec, key: str):\n",
    "    embeddings = []\n",
    "    for row in df.itertuples():\n",
    "        val = getattr(row, key)\n",
    "        embed = w2v_model.wv.get_vector(val, norm=True)\n",
    "        embeddings.append(embed)\n",
    "    return np.array(embeddings)\n",
    "\n",
    "def concat_df_cols(original_df: pd.DataFrame, to_add: np.ndarray, suffix: str):\n",
    "    assert to_add.ndim == 2, \"to_add must have exactly 2 dimensions\"\n",
    "    to_add_df = pd.DataFrame(to_add, columns=[\"%s%d\" % (suffix, i) for i in range(to_add.shape[1])])\n",
    "    merged_df = pd.concat(\n",
    "        (original_df.reset_index(), to_add_df),\n",
    "        axis=1\n",
    "    )\n",
    "    return merged_df.drop(\"index\", axis=1, errors=\"ignore\")\n",
    "product_model = w2v_emb(final_df, \"order_id\", \"product_id\")\n",
    "user_model = w2v_emb(final_df, \"product_id\", \"user_id\")\n",
    "store_model = w2v_emb(final_df, \"product_id\", \"store_id\")\n",
    "\n",
    "# Création des tableaux de contingence\n",
    "final_df = concat_df_cols(final_df, add_embeddings(final_df, product_model, \"product_id\"), \"P\")\n",
    "final_df = concat_df_cols(final_df, add_embeddings(final_df, user_model, \"user_id\"), \"U\")\n",
    "final_df = concat_df_cols(final_df, add_embeddings(final_df, store_model, \"store_id\"), \"S\")\n",
    "# Supprime les attributs inutiles\n",
    "to_drop = [\"order_id\", \"product_id\", \"user_id\", \"store_id\"]\n",
    "final_df = final_df.drop(to_drop, axis=1)\n",
    "\n",
    "# Normalisation des autres attributs\n",
    "scaler = MinMaxScaler()\n",
    "to_scale = ['special', 'order_dow', 'order_number', 'days_since_prior_order', 'order_hour_of_day', 'distance', 'add_to_cart_order', 'reordered']\n",
    "final_df[to_scale] = scaler.fit_transform(final_df[to_scale])\n",
    "# On déplace l'attribut `reordered`, qui sert d'étiquette de prédiction, à fin du tableau final des données\n",
    "final_df = final_df[final_df.drop(\"reordered\", axis=1).columns.tolist() + [\"reordered\"]]\n",
    "# Sauvegarde des données\n",
    "final_df.to_csv(\"./data/train_orders.csv\", index=False)\n",
    "data = final_df.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expérimentations\n",
    "Dans cette section, nous détaillons la méthodologie de nos expériences.\n",
    "\n",
    "## Modèles\n",
    "Nous avons implémenté trois modèles de réseaux de neurones profonds. Les implémentations ainsi que le code d'entraînement se trouvent dans le fichier `models.py`:\n",
    "- MLP5: Un \"multi-layer perceptron\" avec cinq couches cachées, une fonction d'activation \"ReLU\" entre chaque couche et du \"Dropout\" avec 10% de probabilité;\n",
    "- MLP3: Un \"multi-layer perceptron\" avec trois couches cachées, une fonction d'activation \"ReLU\" entre chaque couche et du \"Dropout\" avec 10% de probabilité;\n",
    "- MLP2: Un \"multi-layer perceptron\" avec deux couches cachées, avec une fonction d'activation \"ReLU\" entre chaque couche et du \"Dropout\" avec 10% de probabilité;\n",
    "\n",
    "## Recherche d'hyper-paramètres\n",
    "Nous avons effectué une recherche d'hyper-paramètres à partir de toutes les combinaisons des paramètres ci-dessous:\n",
    "- taux d'apprentissage (learning rate): $(0.001, 0.05, 0.01)$;\n",
    "- taille de lot (batch size) $(32, 64, 128)$;\n",
    "\n",
    "pour un total de neuf combinaisons totales. Pour chaque recherche, nous conservons les hyper-paramètres générant le meilleur F1-Score sur 10 epochs d'entraînement. Voici les résultats de cette recherche d'hyper-paramètres sur nos modèles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "|        | Learning Rate   | Batch Size   | F1-Score   |\n",
    "|--------|-----------------|--------------|------------|\n",
    "| MLP2   | **0.001**       | **32**       | **0.8873** |\n",
    "| MLP2   | 0.001           | 64           | 0.8463     |\n",
    "| MLP2   | 0.001           | 128          | 0.8567     |\n",
    "| MLP2   | 0.05            | 32           | 0.7367     |\n",
    "| MLP2   | 0.05            | 64           | 0.7870     |\n",
    "| MLP2   | 0.05            | 128          | 0.8569     |\n",
    "| MLP2   | 0.01            | 32           | 0.8201     |\n",
    "| MLP2   | 0.01            | 64           | 0.8473     |\n",
    "| MLP2   | 0.01            | 128          | 0.8510     |\n",
    "| ------ | --------------- | ------------ | ---------  |\n",
    "| MLP3   | 0.001           | 32           | 0.8346     |\n",
    "| MLP3   | 0.001           | 64           | 0.8213     |\n",
    "| MLP3   | 0.001           | 128          | 0.8099     |\n",
    "| MLP3   | 0.05            | 32           | 0.8232     |\n",
    "| MLP3   | 0.05            | 64           | 0.8333     |\n",
    "| MLP3   | 0.05            | 128          | 0.8171     |\n",
    "| MLP3   | 0.01            | 32           | 0.8509     |\n",
    "| MLP3   | **0.01**        | **64**       | **0.8568** |\n",
    "| MLP3   | 0.01            | 128          | 0.8234     |\n",
    "| ------ | --------------- | ------------ | ---------  |\n",
    "| MLP5   | 0.001           | 32           | 0.8361     |\n",
    "| MLP5   | 0.001           | 64           | 0.8561     |\n",
    "| MLP5   | 0.001           | 128          | 0.8621     |\n",
    "| MLP5   | 0.05            | 32           | 0.8622     |\n",
    "| MLP5   | **0.05**        | **64**       | **0.8739** |\n",
    "| MLP5   | 0.05            | 128          | 0.8736     |\n",
    "| MLP5   | 0.01            | 32           | 0.8735     |\n",
    "| MLP5   | 0.01            | 64           | 0.8610     |\n",
    "| MLP5   | 0.01            | 128          | 0.8585     |\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Entraînement\n",
    "À partir des hyper-paramètres trouvés à l'étape précédente, chaque modèle est entraîné $10$ fois pour $100$ epochs avec l'algorithme d'optimisation Adam et la fonction de perte d'entropie croisée. Pour chaque entraînement, 70% des données sont réservées pour l'entraînement et le reste (30%) sont données à l'ensemble de test. Lors de l'étape de nettoyage des données, nous avons éliminé les produits et clients rares en plus de conserver un ratio équivalent de commandes pour chaque client. Par conséquent, on peut se permettre un échantillonnage aléatoire lors de l'entraînement. De plus, on effectue $10$ entraînements sur des échantillons différents, ce qui minimise le risque de tomber sur un échantillon biaisé. On rapporte la moyenne et l'écart-type de chaque entraînement. Un grand écart-type implique que notre modèle est très sensible à l'échantillonnage effectué; à l'opposé, un petit écart-type signifie le contraire."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Résultats\n",
    "Les résultats avec les paramètres optimaux pour chaque modèle sont présentés ci-dessous. Les métriques de performance ont été calculées avec deux classes d'intérêt différentes: la classe minoritaire (`reordered=0`) et la classe majoritaire (`reordered=1`). On priorise les performances sur la classe `reordered=1` car nous sommes intéressés par les capacités prédictives de notre modèle sur cette classe."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "            Precision           Recall         F1-Score    lr  batch_size\nMLP5  0.4154 (0.1407)  0.5422 (0.1940)  0.4676 (0.1582)  0.01        64.0\nMLP3  0.4932 (0.0221)  0.6661 (0.0438)  0.5651 (0.0035)  0.01        32.0\nMLP2  0.5114 (0.0217)  0.6345 (0.0406)  0.5648 (0.0053)  0.01        64.0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1-Score</th>\n      <th>lr</th>\n      <th>batch_size</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>MLP5</th>\n      <td>0.4154 (0.1407)</td>\n      <td>0.5422 (0.1940)</td>\n      <td>0.4676 (0.1582)</td>\n      <td>0.01</td>\n      <td>64.0</td>\n    </tr>\n    <tr>\n      <th>MLP3</th>\n      <td>0.4932 (0.0221)</td>\n      <td>0.6661 (0.0438)</td>\n      <td>0.5651 (0.0035)</td>\n      <td>0.01</td>\n      <td>32.0</td>\n    </tr>\n    <tr>\n      <th>MLP2</th>\n      <td>0.5114 (0.0217)</td>\n      <td>0.6345 (0.0406)</td>\n      <td>0.5648 (0.0053)</td>\n      <td>0.01</td>\n      <td>64.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Résultats lorsque la classe d'intérêt est `reordered=0`\n",
    "results_df = pd.read_csv(\"./results/instacart_results_pos_label_0.csv\", index_col=0)\n",
    "results_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "            Precision           Recall         F1-Score     lr  batch_size\nMLP5  0.7745 (0.0009)  1.0000 (0.0000)  0.8729 (0.0006)  0.050        64.0\nMLP3  0.8784 (0.0088)  0.7813 (0.0305)  0.8266 (0.0153)  0.010        64.0\nMLP2  0.8882 (0.0057)  0.8151 (0.0221)  0.8498 (0.0099)  0.001        32.0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1-Score</th>\n      <th>lr</th>\n      <th>batch_size</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>MLP5</th>\n      <td>0.7745 (0.0009)</td>\n      <td>1.0000 (0.0000)</td>\n      <td>0.8729 (0.0006)</td>\n      <td>0.050</td>\n      <td>64.0</td>\n    </tr>\n    <tr>\n      <th>MLP3</th>\n      <td>0.8784 (0.0088)</td>\n      <td>0.7813 (0.0305)</td>\n      <td>0.8266 (0.0153)</td>\n      <td>0.010</td>\n      <td>64.0</td>\n    </tr>\n    <tr>\n      <th>MLP2</th>\n      <td>0.8882 (0.0057)</td>\n      <td>0.8151 (0.0221)</td>\n      <td>0.8498 (0.0099)</td>\n      <td>0.001</td>\n      <td>32.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Résultats lorsque la classe d'intérêt est `reordered=1`\n",
    "results_df = pd.read_csv(\"./results/instacart_results_pos_label_1.csv\", index_col=0)\n",
    "results_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Interprétation\n",
    "Sans surprise, les résultats sont nettement supérieurs lorsqu'on considère la classe majoritaire comme la classe d'intérêt. Étant présentes dans une plus grande proportion, ces données sont moins risquées à prédire. Il est généralement reconnu que plus le modèle est profond, plus celui-ci est performant. Or cette intuition n'est pas confirmée par les résultats actuels. Le modèle plus profond, avec cinq couches cachées, présente les meilleurs résultats sur la classe majoritaire, mais génère en même temps les moins bons résultats sur la classe minoritaire avec une différence de $10\\%$ en termes de F1-Score comparativement aux modèles à trois et deux couches. Clairement, MLP5 est plus performant pour reconnaître les achats déjà effectués par le passé que l'inverse. On note aussi un grand écart-type pour les résultats de ce modèle sur la classe minoritaire. Par contraste, les modèles MLP3 et MLP2 performent de manière similaire en termes de F1-Score, Precision et Recall, peu importe la classe d'intérêt.\n",
    "\n",
    "Dans tous les cas, le niveau de Precision indique que notre modèle génère peu de faux positifs, c'est-à-dire qu'il identifie des produits qui n'ont jamais été commandés par le client. Le Recall du MLP5 est parfait, c'est-à-dire qu'il ne génère aucun faux négatif. Dans notre contexte, cela implique que notre modèle identifie toutes les classes minoritaires correctement. Le F1-Score ($0.8729$), dans ce cas, n'est pas très informatif, car il cache le fait que le Recall est parfait tandis que la Precision ne l'est pas. Il nous indique seulement que le modèle est bien balancé entre les faux positifs et les faux négatifs."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "98721178b2fa100978e69bf0600317bd2bb664c245b34d5c4cc5c099a20743a9"
  },
  "kernelspec": {
   "display_name": "Python 3.6.15 ('ift780')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}